{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.11.0\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import numpy as np\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "print(cv2.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# placing the data into a pandas dataframe\n",
    "\n",
    "\n",
    "def createDF(path):\n",
    "    file_name = []\n",
    "    animal = []\n",
    "    # training = Path(path)\n",
    "    print(path)\n",
    "    training = os.listdir(path)\n",
    "    print(training)\n",
    "\n",
    "    for folder in training:\n",
    "        folderpath = os.path.join(path,folder)\n",
    "        if folder == '.gitkeep':\n",
    "            continue\n",
    "        files = os.listdir(folderpath)\n",
    "        for file in files:\n",
    "            filepath = os.path.join(folderpath,file)\n",
    "            filepath = filepath.replace('\\\\','/')\n",
    "            file_name.append(filepath)\n",
    "            if(folder == 'cats'):\n",
    "                animal.append('0')  \n",
    "            else:\n",
    "                animal.append('1')  \n",
    "\n",
    "    print(len(file_name))\n",
    "    print(len(animal))\n",
    "\n",
    "    df = pd.DataFrame({\n",
    "        'file_name': file_name,\n",
    "        'animal': animal\n",
    "    })\n",
    "    # shuffle\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    print(df.head(3))\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data/training_set/training_set\n",
      "['.gitkeep', 'cats', 'dogs']\n",
      "8005\n",
      "8005\n",
      "                                           file_name animal\n",
      "0  ../data/training_set/training_set/cats/cat.139...      0\n",
      "1  ../data/training_set/training_set/dogs/dog.386...      1\n",
      "2  ../data/training_set/training_set/cats/cat.166...      0\n",
      "../data/test_set/test_set\n",
      "['.gitkeep', 'cats', 'dogs']\n",
      "2023\n",
      "2023\n",
      "                                     file_name animal\n",
      "0  ../data/test_set/test_set/dogs/dog.4319.jpg      1\n",
      "1  ../data/test_set/test_set/dogs/dog.4049.jpg      1\n",
      "2  ../data/test_set/test_set/dogs/dog.4192.jpg      1\n"
     ]
    }
   ],
   "source": [
    "train_df = createDF(\"../data/training_set/training_set\")\n",
    "train_df.to_csv(\"../data/csv/train_data.csv\")\n",
    "train_df.shape\n",
    "\n",
    "test_df = createDF(\"../data/test_set/test_set\")\n",
    "test_df.to_csv(\"../data/csv/test_data.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # sirbins threading lezgoooo\n",
    "# df = pd.read_csv(\"../data/csv/train_data.csv\")\n",
    "\n",
    "# # Take the first 50 samples and split into 6 batches\n",
    "# # testing = df.head(50)\n",
    "\n",
    "# # half of cores buot ka\n",
    "# threads = math.ceil(os.cpu_count() / 2)\n",
    "# arr2 = np.array_split(df, threads)\n",
    "# print(f\"threads {threads}\")\n",
    "\n",
    "# # Function to process a batch of images\n",
    "# def process(images):\n",
    "#     animal_type = []\n",
    "#     image_arr = []\n",
    "\n",
    "#     for _, row in images.iterrows():  \n",
    "#         animal = row['animal']  # Get animal (0 = cat, 1 = dog)\n",
    "#         animal_type.append(animal)\n",
    "\n",
    "#         # Read and process the image\n",
    "#         image = cv2.imread(row['file_name'])\n",
    "#         image = cv2.resize(image, (224, 224))\n",
    "#         normalized = image / 255.0  # Normalize (0-1)\n",
    "#         image_arr.append(normalized)\n",
    "\n",
    "#     return animal_type, image_arr\n",
    "\n",
    "# # Parallel processing\n",
    "# arrs = []\n",
    "# with ThreadPoolExecutor(max_workers=threads) as ex:\n",
    "#     futures = [ex.submit(process, batch) for batch in arr2] \n",
    "\n",
    "#     for future in futures:\n",
    "#         arrs.append(future.result())  \n",
    "\n",
    "# final_type = []\n",
    "# final_images = []\n",
    "\n",
    "# for animal, images in arrs:\n",
    "#     final_type.extend(animal)\n",
    "#     final_images.extend(images)\n",
    "\n",
    "# # Convert to NumPy arrays\n",
    "# X_test = np.array(final_images)\n",
    "# y_test = np.array(final_type)\n",
    "\n",
    "# # Save as .npy\n",
    "# np.save(\"../data/X_test.npy\", X_test)\n",
    "# np.save(\"../data/y_test.npy\", y_test)\n",
    "\n",
    "# print(f\"! Shape: {X_test.shape} {y_test.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess yarn hehe\n",
    "\n",
    "train = pd.read_csv(\"../data/csv/train_data.csv\")\n",
    "\n",
    "# train = pd.read_csv(\"../data/csv/test_data.csv\")\n",
    "# train = df.head(3)\n",
    "# print(train)\n",
    "animal_type =[]\n",
    "image_arr = []\n",
    "# TODO: RESEARCH THREADPOOLEXECUTOR\n",
    "for i in range(len(train)):\n",
    "\n",
    "    animal = train.iloc[i]['animal'] # gets the type of animal\n",
    "    animal_type.append(animal)\n",
    "\n",
    "    # print(train.iloc[i]['file_name'])\n",
    "    image = cv2.imread(train.iloc[i]['file_name']) \n",
    "\n",
    "    # resize image 224 which is common and normalize to make it 0-1\n",
    "    # image = cv2.resize(image,(224,224)) \n",
    "    image = cv2.resize(image,(150,150)) \n",
    "    normalized = image / 255.0\n",
    "    image_arr.append(normalized)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  \n",
    "\n",
    "# save as numpy mas efficient kaysa csv unsay reason? ambot nimo ayaw pagbuot char researching..\n",
    "x = np.array(image_arr,dtype=np.float16)\n",
    "y = np.array(animal_type,dtype=np.int8)\n",
    "\n",
    "# TAPOL VERSION UNCOMMENT WHICH ONE YOU WANT\n",
    "\n",
    "train_dataset = Dataset.from_tensor_slices((x, y)).batch(16)\n",
    "Dataset.save(train_dataset,'../data/tfdata/saved_training')\n",
    "\n",
    "# test_dataset = Dataset.from_tensor_slices((x_test, y_test)).batch(16)\n",
    "# Dataset.save(test_dataset,'../data/tfdata/saved_testing')\n",
    "\n",
    "\n",
    "# DEPRECATEEEED\n",
    "# x_train = np.save(\"../data/numpy/X_train.npy\",x)\n",
    "# Y_train = np.save(\"../data/numpy/Y_train.npy\",y)\n",
    "\n",
    "# x_train = np.save(\"../data/numpy/X_test.npy\",x)\n",
    "# Y_train = np.save(\"../data/numpy/Y_test.npy\",y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for x_batch, y_batch in train_dataset.take(1):  # Take 1 batch\n",
    "    print(\"x_batch shape:\", x_batch.shape)\n",
    "    print(\"y_batch shape:\", y_batch.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf217",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
